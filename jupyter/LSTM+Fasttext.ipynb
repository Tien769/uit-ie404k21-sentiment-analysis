{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import text, sequence \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_LEN = 20\n",
    "TRUNC_TYPE = 'post'\n",
    "PADDING_TYPE = 'post'\n",
    "OOV_TOKEN = '<OOV>'\n",
    "LABEL_NUMBER = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv('../data/train.csv') # for training\n",
    "# test_data = pd.read_csv('../data/test.csv') # for testing\n",
    "train_data = pd.read_csv('../data/train_mod.csv') # for training\n",
    "test_data = pd.read_csv('../data/test_mod.csv') # for testing\n",
    "# train_data = pd.read_csv('../data/train_mod_3labels.csv') # for training\n",
    "# test_data = pd.read_csv('../data/test_mod_3labels.csv') # for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tokenizer and word_index\n",
    "sentence_tokenizer = text.Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN)\n",
    "sentence_tokenizer.fit_on_texts(train_data.sentence.values)\n",
    "word_index = sentence_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "number of null word embeddings: 1144\n"
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "ft = io.open('../pretrained/cc.vi.300.vec', encoding='utf-8')\n",
    "\n",
    "embeddings_index = {}\n",
    "for line in ft:\n",
    "    values = line.rstrip().split(' ')\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = vector\n",
    "\n",
    "ft.close()\n",
    "\n",
    "words_not_found = []\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "        \n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentence\n",
    "train_sentence = sentence_tokenizer.texts_to_sequences(train_data.sentence.values) # Convert all word to sequence\n",
    "train_sentence = sequence.pad_sequences(train_sentence, maxlen=MAX_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE) # Pad each entry\n",
    "test_sentence = sentence_tokenizer.texts_to_sequences(test_data.sentence.values) # Convert all word to sequence\n",
    "test_sentence = sequence.pad_sequences(test_sentence, maxlen=MAX_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE) # Pad each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize emotion\n",
    "train_emotion = pd.get_dummies(train_data.emotion.values)\n",
    "test_emotion = pd.get_dummies(test_data.emotion.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (None, 20, 300)           1500000   \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 20, 300)           0         \n_________________________________________________________________\nbidirectional_3 (Bidirection (None, 256)               439296    \n_________________________________________________________________\ndense_6 (Dense)              (None, 64)                16448     \n_________________________________________________________________\ndense_7 (Dense)              (None, 7)                 455       \n=================================================================\nTotal params: 1,956,199\nTrainable params: 456,199\nNon-trainable params: 1,500,000\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_LEN, trainable=False),\n",
    "    # layers.SpatialDropout1D(.5),\n",
    "    layers.Dropout(.5),\n",
    "    layers.Bidirectional(layers.LSTM(128)),\n",
    "    layers.Dense(64, activation='sigmoid'),\n",
    "    # layers.Dropout(.5),\n",
    "    layers.Dense(LABEL_NUMBER, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 4993 samples, validate on 555 samples\nEpoch 1/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.8111 - accuracy: 0.2708 - precision: 0.0541 - recall: 4.0064e-04\nEpoch 00001: val_accuracy improved from -inf to 0.32252, saving model to LSTMV2.h5\n4993/4993 [==============================] - 18s 4ms/sample - loss: 1.8111 - accuracy: 0.2708 - precision: 0.0541 - recall: 4.0056e-04 - val_loss: 1.7140 - val_accuracy: 0.3225 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\nEpoch 2/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.6967 - accuracy: 0.3231 - precision: 0.6000 - recall: 0.0156\nEpoch 00002: val_accuracy improved from 0.32252 to 0.34955, saving model to LSTMV2.h5\n4993/4993 [==============================] - 10s 2ms/sample - loss: 1.6968 - accuracy: 0.3231 - precision: 0.6000 - recall: 0.0156 - val_loss: 1.6238 - val_accuracy: 0.3495 - val_precision: 1.0000 - val_recall: 0.0054\nEpoch 3/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.6214 - accuracy: 0.3660 - precision: 0.6583 - recall: 0.0367\nEpoch 00003: val_accuracy improved from 0.34955 to 0.39099, saving model to LSTMV2.h5\n4993/4993 [==============================] - 10s 2ms/sample - loss: 1.6214 - accuracy: 0.3659 - precision: 0.6583 - recall: 0.0367 - val_loss: 1.5637 - val_accuracy: 0.3910 - val_precision: 0.6957 - val_recall: 0.1153\nEpoch 4/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.5552 - accuracy: 0.3872 - precision: 0.6222 - recall: 0.0841\nEpoch 00004: val_accuracy improved from 0.39099 to 0.42703, saving model to LSTMV2.h5\n4993/4993 [==============================] - 10s 2ms/sample - loss: 1.5552 - accuracy: 0.3873 - precision: 0.6222 - recall: 0.0841 - val_loss: 1.5334 - val_accuracy: 0.4270 - val_precision: 0.8125 - val_recall: 0.0468\nEpoch 5/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.4963 - accuracy: 0.4287 - precision: 0.6524 - recall: 0.1068\nEpoch 00005: val_accuracy improved from 0.42703 to 0.44685, saving model to LSTMV2.h5\n4993/4993 [==============================] - 10s 2ms/sample - loss: 1.4963 - accuracy: 0.4286 - precision: 0.6524 - recall: 0.1067 - val_loss: 1.4464 - val_accuracy: 0.4468 - val_precision: 0.6899 - val_recall: 0.1604\nEpoch 6/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.4559 - accuracy: 0.4373 - precision: 0.6279 - recall: 0.1480\nEpoch 00006: val_accuracy did not improve from 0.44685\n4993/4993 [==============================] - 10s 2ms/sample - loss: 1.4559 - accuracy: 0.4372 - precision: 0.6279 - recall: 0.1480 - val_loss: 1.4488 - val_accuracy: 0.4324 - val_precision: 0.7500 - val_recall: 0.1351\nEpoch 7/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.4114 - accuracy: 0.4551 - precision: 0.6579 - recall: 0.1695\nEpoch 00007: val_accuracy improved from 0.44685 to 0.47387, saving model to LSTMV2.h5\n4993/4993 [==============================] - 10s 2ms/sample - loss: 1.4115 - accuracy: 0.4550 - precision: 0.6573 - recall: 0.1694 - val_loss: 1.4116 - val_accuracy: 0.4739 - val_precision: 0.7323 - val_recall: 0.1676\nEpoch 8/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.3677 - accuracy: 0.4768 - precision: 0.6667 - recall: 0.1979\nEpoch 00008: val_accuracy did not improve from 0.47387\n4993/4993 [==============================] - 10s 2ms/sample - loss: 1.3676 - accuracy: 0.4769 - precision: 0.6669 - recall: 0.1981 - val_loss: 1.4156 - val_accuracy: 0.4577 - val_precision: 0.6346 - val_recall: 0.2378\nEpoch 9/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.3545 - accuracy: 0.4872 - precision: 0.6470 - recall: 0.2236\nEpoch 00009: val_accuracy improved from 0.47387 to 0.48829, saving model to LSTMV2.h5\n4993/4993 [==============================] - 11s 2ms/sample - loss: 1.3544 - accuracy: 0.4873 - precision: 0.6470 - recall: 0.2235 - val_loss: 1.3483 - val_accuracy: 0.4883 - val_precision: 0.7207 - val_recall: 0.2324\nEpoch 10/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.3416 - accuracy: 0.4922 - precision: 0.6625 - recall: 0.2438\nEpoch 00010: val_accuracy improved from 0.48829 to 0.49730, saving model to LSTMV2.h5\n4993/4993 [==============================] - 13s 3ms/sample - loss: 1.3417 - accuracy: 0.4921 - precision: 0.6625 - recall: 0.2437 - val_loss: 1.3488 - val_accuracy: 0.4973 - val_precision: 0.7181 - val_recall: 0.2432\nEpoch 11/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.3069 - accuracy: 0.5024 - precision: 0.6811 - recall: 0.2558\nEpoch 00011: val_accuracy did not improve from 0.49730\n4993/4993 [==============================] - 13s 3ms/sample - loss: 1.3067 - accuracy: 0.5025 - precision: 0.6812 - recall: 0.2560 - val_loss: 1.3501 - val_accuracy: 0.4937 - val_precision: 0.6772 - val_recall: 0.2306\nEpoch 12/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.2751 - accuracy: 0.5192 - precision: 0.6788 - recall: 0.2833\nEpoch 00012: val_accuracy did not improve from 0.49730\n4993/4993 [==============================] - 12s 2ms/sample - loss: 1.2751 - accuracy: 0.5193 - precision: 0.6788 - recall: 0.2832 - val_loss: 1.3296 - val_accuracy: 0.4919 - val_precision: 0.6812 - val_recall: 0.2811\nEpoch 13/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.2591 - accuracy: 0.5236 - precision: 0.6725 - recall: 0.2991\nEpoch 00013: val_accuracy improved from 0.49730 to 0.50811, saving model to LSTMV2.h5\n4993/4993 [==============================] - 12s 2ms/sample - loss: 1.2589 - accuracy: 0.5237 - precision: 0.6727 - recall: 0.2992 - val_loss: 1.3121 - val_accuracy: 0.5081 - val_precision: 0.6872 - val_recall: 0.2811\nEpoch 14/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.2457 - accuracy: 0.5306 - precision: 0.6782 - recall: 0.3065\nEpoch 00014: val_accuracy did not improve from 0.50811\n4993/4993 [==============================] - 12s 2ms/sample - loss: 1.2456 - accuracy: 0.5307 - precision: 0.6782 - recall: 0.3064 - val_loss: 1.3030 - val_accuracy: 0.5009 - val_precision: 0.6452 - val_recall: 0.2883\nEpoch 15/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.2025 - accuracy: 0.5411 - precision: 0.6923 - recall: 0.3375\nEpoch 00015: val_accuracy did not improve from 0.50811\n4993/4993 [==============================] - 11s 2ms/sample - loss: 1.2024 - accuracy: 0.5412 - precision: 0.6924 - recall: 0.3377 - val_loss: 1.3279 - val_accuracy: 0.4703 - val_precision: 0.6383 - val_recall: 0.3243\nEpoch 16/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.1877 - accuracy: 0.5565 - precision: 0.6879 - recall: 0.3484\nEpoch 00016: val_accuracy did not improve from 0.50811\n4993/4993 [==============================] - 14s 3ms/sample - loss: 1.1876 - accuracy: 0.5566 - precision: 0.6880 - recall: 0.3485 - val_loss: 1.3169 - val_accuracy: 0.5027 - val_precision: 0.6567 - val_recall: 0.3171\nEpoch 17/30\n4992/4993 [============================>.] - ETA: 0s - loss: 1.1711 - accuracy: 0.5621 - precision: 0.6973 - recall: 0.3544\nEpoch 00017: val_accuracy did not improve from 0.50811\n4993/4993 [==============================] - 10s 2ms/sample - loss: 1.1711 - accuracy: 0.5622 - precision: 0.6973 - recall: 0.3543 - val_loss: 1.3258 - val_accuracy: 0.4883 - val_precision: 0.6396 - val_recall: 0.3261\nEpoch 00017: early stopping\n"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]])\n",
    "\n",
    "initial_epochs = 30\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)\n",
    "mc=ModelCheckpoint('LSTMV2.h5', monitor='val_accuracy', mode='max', save_best_only=True,verbose=1) \n",
    "history = model.fit(train_sentence, train_emotion,batch_size=64, epochs=initial_epochs, validation_split=.1, verbose=1, callbacks=[es,mc])\n",
    "# history = model.fit(train_sentence, train_emotion,batch_size=64, epochs=initial_epochs, validation_split=.1, verbose=1)\n",
    "# model.evaluate(test_sentence, test_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "693/693 [==============================] - 1s 869us/sample - loss: 1.3290 - accuracy: 0.5007 - precision: 0.6548 - recall: 0.2655\n[1.3289614133160523, 0.5007215, 0.6548043, 0.26551226]\n"
    }
   ],
   "source": [
    "model.load_weights('LSTMV2.h5')\n",
    "result = model.evaluate(test_sentence, test_emotion)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.00918274 0.0775242  0.55239874 0.01682269 0.06917091 0.27490067]]\nEnjoyment\n"
    }
   ],
   "source": [
    "sentence = 'Thật là kinh khủng'\n",
    "# labels = ['Anger', 'Disgust', 'Enjoyment', 'Fear', 'Other', 'Sadness', 'Surprise']\n",
    "labels = ['Anger', 'Disgust', 'Enjoyment', 'Fear', 'Sadness', 'Surprise']\n",
    "# labels = ['Negative', 'Neutral', 'Positive']\n",
    "sentence = sentence_tokenizer.texts_to_sequences([sentence])\n",
    "sentence = sequence.pad_sequences(sentence, maxlen=MAX_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n",
    "pred = model.predict([sentence])\n",
    "print(pred) \n",
    "print(labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import text, sequence \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_LEN = 20\n",
    "TRUNC_TYPE = 'post'\n",
    "PADDING_TYPE = 'post'\n",
    "OOV_TOKEN = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/train_mod.csv') # for training\n",
    "test_data = pd.read_csv('../data/test_mod.csv') # for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tokenizer and word_index\n",
    "sentence_tokenizer = text.Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN)\n",
    "sentence_tokenizer.fit_on_texts(train_data.sentence.values)\n",
    "word_index = sentence_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "number of null word embeddings: 1144\n"
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "ft = io.open('../pretrained/cc.vi.300.vec', encoding='utf-8')\n",
    "\n",
    "embeddings_index = {}\n",
    "for line in ft:\n",
    "    values = line.rstrip().split(' ')\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = vector\n",
    "\n",
    "ft.close()\n",
    "\n",
    "words_not_found = []\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "        \n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentence\n",
    "train_sentence = sentence_tokenizer.texts_to_sequences(train_data.sentence.values) # Convert all word to sequence\n",
    "train_sentence = sequence.pad_sequences(train_sentence, maxlen=MAX_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE) # Pad each entry\n",
    "test_sentence = sentence_tokenizer.texts_to_sequences(test_data.sentence.values) # Convert all word to sequence\n",
    "test_sentence = sequence.pad_sequences(test_sentence, maxlen=MAX_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE) # Pad each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize emotion\n",
    "train_emotion = pd.get_dummies(train_data.emotion.values)\n",
    "test_emotion = pd.get_dummies(test_data.emotion.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 20, 300)           1500000   \n_________________________________________________________________\ndropout (Dropout)            (None, 20, 300)           0         \n_________________________________________________________________\nbidirectional (Bidirectional (None, 256)               439296    \n_________________________________________________________________\ndense (Dense)                (None, 64)                16448     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 6)                 390       \n=================================================================\nTotal params: 1,956,134\nTrainable params: 456,134\nNon-trainable params: 1,500,000\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_LEN, trainable=False),\n",
    "    # layers.SpatialDropout1D(.5),\n",
    "    layers.Dropout(.5),\n",
    "    layers.Bidirectional(layers.LSTM(128)),\n",
    "    layers.Dense(64, activation='sigmoid'),\n",
    "    layers.Dropout(.5),\n",
    "    layers.Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 4074 samples, validate on 453 samples\nEpoch 1/30\n4032/4074 [============================>.] - ETA: 0s - loss: 1.1481 - accuracy: 0.5615\nEpoch 00001: val_accuracy improved from -inf to 0.52980, saving model to LSTMV2.h5\n4074/4074 [==============================] - 20s 5ms/sample - loss: 1.1470 - accuracy: 0.5626 - val_loss: 1.1555 - val_accuracy: 0.5298\nEpoch 2/30\n4032/4074 [============================>.] - ETA: 0s - loss: 1.1077 - accuracy: 0.5813\nEpoch 00002: val_accuracy improved from 0.52980 to 0.59161, saving model to LSTMV2.h5\n4074/4074 [==============================] - 8s 2ms/sample - loss: 1.1098 - accuracy: 0.5805 - val_loss: 1.1104 - val_accuracy: 0.5916\nEpoch 3/30\n4032/4074 [============================>.] - ETA: 0s - loss: 1.0841 - accuracy: 0.5878\nEpoch 00003: val_accuracy did not improve from 0.59161\n4074/4074 [==============================] - 9s 2ms/sample - loss: 1.0845 - accuracy: 0.5874 - val_loss: 1.1156 - val_accuracy: 0.5717\nEpoch 4/30\n4032/4074 [============================>.] - ETA: 0s - loss: 1.0873 - accuracy: 0.5928\nEpoch 00004: val_accuracy did not improve from 0.59161\n4074/4074 [==============================] - 10s 3ms/sample - loss: 1.0892 - accuracy: 0.5928 - val_loss: 1.1145 - val_accuracy: 0.5629\nEpoch 5/30\n4032/4074 [============================>.] - ETA: 0s - loss: 1.0814 - accuracy: 0.5843\nEpoch 00005: val_accuracy improved from 0.59161 to 0.59823, saving model to LSTMV2.h5\n4074/4074 [==============================] - 11s 3ms/sample - loss: 1.0800 - accuracy: 0.5849 - val_loss: 1.0941 - val_accuracy: 0.5982\nEpoch 6/30\n4032/4074 [============================>.] - ETA: 0s - loss: 1.0499 - accuracy: 0.6039\nEpoch 00006: val_accuracy did not improve from 0.59823\n4074/4074 [==============================] - 9s 2ms/sample - loss: 1.0553 - accuracy: 0.6014 - val_loss: 1.1087 - val_accuracy: 0.5850\nEpoch 7/30\n4032/4074 [============================>.] - ETA: 0s - loss: 1.0373 - accuracy: 0.6101\nEpoch 00007: val_accuracy did not improve from 0.59823\n4074/4074 [==============================] - 10s 3ms/sample - loss: 1.0393 - accuracy: 0.6085 - val_loss: 1.0971 - val_accuracy: 0.5717\nEpoch 8/30\n4032/4074 [============================>.] - ETA: 0s - loss: 1.0075 - accuracy: 0.6193\nEpoch 00008: val_accuracy did not improve from 0.59823\n4074/4074 [==============================] - 10s 2ms/sample - loss: 1.0081 - accuracy: 0.6190 - val_loss: 1.1446 - val_accuracy: 0.5651\nEpoch 00008: early stopping\n"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "initial_epochs = 30\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)\n",
    "mc=ModelCheckpoint('LSTMV2.h5', monitor='val_accuracy', mode='max', save_best_only=True,verbose=1) \n",
    "history = model.fit(train_sentence, train_emotion,batch_size=64, epochs=initial_epochs, validation_split=.1, verbose=1, callbacks=[es,mc])\n",
    "# history = model.fit(train_sentence, train_emotion,batch_size=64, epochs=initial_epochs, validation_split=.1, verbose=1)\n",
    "# model.evaluate(test_sentence, test_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "564/564 [==============================] - 1s 1ms/sample - loss: 1.1131 - accuracy: 0.5780\n[1.1131218350525443, 0.5780142]\n"
    }
   ],
   "source": [
    "model.load_weights('LSTMV2.h5')\n",
    "result = model.evaluate(test_sentence, test_emotion)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.00918274 0.0775242  0.55239874 0.01682269 0.06917091 0.27490067]]\nEnjoyment\n"
    }
   ],
   "source": [
    "sentence = 'Thật là kinh khủng'\n",
    "# labels = ['Anger', 'Disgust', 'Enjoyment', 'Fear', 'Other', 'Sadness', 'Surprise']\n",
    "labels = ['Anger', 'Disgust', 'Enjoyment', 'Fear', 'Sadness', 'Surprise']\n",
    "# labels = ['Negative', 'Neutral', 'Positive']\n",
    "sentence = sentence_tokenizer.texts_to_sequences([sentence])\n",
    "sentence = sequence.pad_sequences(sentence, maxlen=MAX_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n",
    "pred = model.predict([sentence])\n",
    "print(pred) \n",
    "print(labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import gensim\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import text, sequence \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "EMBEDDING_DIM = 400\n",
    "MAX_LEN = 20\n",
    "TRUNC_TYPE = 'post'\n",
    "PADDING_TYPE = 'post'\n",
    "OOV_TOKEN = '<OOV>'\n",
    "LABEL_NUMBER = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv('../data/train.csv') # for training\n",
    "# test_data = pd.read_csv('../data/test.csv') # for testing\n",
    "# train_data = pd.read_csv('../data/train_mod.csv') # for training\n",
    "# test_data = pd.read_csv('../data/test_mod.csv') # for testing\n",
    "train_data = pd.read_csv('../data/train_mod_3labels.csv') # for training\n",
    "test_data = pd.read_csv('../data/test_mod_3labels.csv') # for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tokenizer and word_index\n",
    "sentence_tokenizer = text.Tokenizer(oov_token=OOV_TOKEN)\n",
    "sentence_tokenizer.fit_on_texts(train_data.sentence.values)\n",
    "word_index = sentence_tokenizer.word_index\n",
    "VOCAB_SIZE = len(word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentence\n",
    "train_sentence = sentence_tokenizer.texts_to_sequences(train_data.sentence.values) # Convert all word to sequence\n",
    "train_sentence = sequence.pad_sequences(train_sentence, maxlen=MAX_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE) # Pad each entry\n",
    "test_sentence = sentence_tokenizer.texts_to_sequences(test_data.sentence.values) # Convert all word to sequence\n",
    "test_sentence = sequence.pad_sequences(test_sentence, maxlen=MAX_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE) # Pad each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize emotion\n",
    "train_emotion = pd.get_dummies(train_data.emotion.values)\n",
    "test_emotion = pd.get_dummies(test_data.emotion.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Null word embeddings: 816\n"
    }
   ],
   "source": [
    "word_model = gensim.models.KeyedVectors.load_word2vec_format('../pretrained/wiki.vi.model.bin', binary=True)\n",
    "\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word_model.vocab:\n",
    "        embedding_matrix[i] = word_model.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4527\n1488\n"
    }
   ],
   "source": [
    "documents = train_data.sentence.to_list()\n",
    "wc_sentences = [[word for word in document.lower().split()] for document in documents]\n",
    "wc_model = gensim.models.Word2Vec(sentences=wc_sentences, size=EMBEDDING_DIM)\n",
    "print(len(wc_sentences))\n",
    "print(len(list(wc_model.wv.vocab)))\n",
    "wc_model.wv.save_word2vec_format('../pretrained/myword2vec', binary=False)\n",
    "embeddings_index = {}\n",
    "\n",
    "wf = open('../pretrained/myword2vec.txt', encoding='utf-8')\n",
    "for line in wf:\n",
    "    values=line.split()\n",
    "    word=values[0]\n",
    "    vector=np.asarray(values[1:])\n",
    "    embeddings_index[word] = vector\n",
    "wf.close()\n",
    "\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > VOCAB_SIZE:\n",
    "        continue\n",
    "    if word in word_model.vocab:\n",
    "        embedding_matrix[i] = word_model.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_7\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_7 (Embedding)      (None, 20, 400)           1684000   \n_________________________________________________________________\ndropout_14 (Dropout)         (None, 20, 400)           0         \n_________________________________________________________________\nbidirectional_7 (Bidirection (None, 256)               541696    \n_________________________________________________________________\ndense_14 (Dense)             (None, 64)                16448     \n_________________________________________________________________\ndropout_15 (Dropout)         (None, 64)                0         \n_________________________________________________________________\ndense_15 (Dense)             (None, 3)                 195       \n=================================================================\nTotal params: 2,242,339\nTrainable params: 558,339\nNon-trainable params: 1,684,000\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_LEN, trainable=False),\n",
    "    # layers.SpatialDropout1D(.5),\n",
    "    layers.Dropout(.5),\n",
    "    layers.Bidirectional(layers.LSTM(128, dropout=.2, recurrent_dropout=.2)),\n",
    "    layers.Dense(64, activation='sigmoid'),\n",
    "    layers.Dropout(.5),\n",
    "    layers.Dense(LABEL_NUMBER, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 4074 samples, validate on 453 samples\nEpoch 1/30\n4032/4074 [============================>.] - ETA: 0s - loss: 0.9264 - accuracy: 0.5486 - precision_6: 0.5713 - recall_6: 0.4807\nEpoch 00001: val_accuracy improved from -inf to 0.64901, saving model to LSTMV3.h5\n4074/4074 [==============================] - 21s 5ms/sample - loss: 0.9263 - accuracy: 0.5488 - precision_6: 0.5714 - recall_6: 0.4811 - val_loss: 0.7730 - val_accuracy: 0.6490 - val_precision_6: 0.6545 - val_recall_6: 0.6313\nEpoch 2/30\n4032/4074 [============================>.] - ETA: 0s - loss: 0.7921 - accuracy: 0.6238 - precision_6: 0.6413 - recall_6: 0.5454\nEpoch 00002: val_accuracy improved from 0.64901 to 0.72627, saving model to LSTMV3.h5\n4074/4074 [==============================] - 14s 3ms/sample - loss: 0.7903 - accuracy: 0.6254 - precision_6: 0.6428 - recall_6: 0.5469 - val_loss: 0.6823 - val_accuracy: 0.7263 - val_precision_6: 0.7435 - val_recall_6: 0.6976\nEpoch 3/30\n4032/4074 [============================>.] - ETA: 0s - loss: 0.7222 - accuracy: 0.6885 - precision_6: 0.7059 - recall_6: 0.6352\nEpoch 00003: val_accuracy improved from 0.72627 to 0.73510, saving model to LSTMV3.h5\n4074/4074 [==============================] - 15s 4ms/sample - loss: 0.7220 - accuracy: 0.6885 - precision_6: 0.7059 - recall_6: 0.6352 - val_loss: 0.6807 - val_accuracy: 0.7351 - val_precision_6: 0.7379 - val_recall_6: 0.7086\nEpoch 4/30\n4032/4074 [============================>.] - ETA: 0s - loss: 0.6849 - accuracy: 0.7081 - precision_6: 0.7279 - recall_6: 0.6615\nEpoch 00004: val_accuracy improved from 0.73510 to 0.74614, saving model to LSTMV3.h5\n4074/4074 [==============================] - 16s 4ms/sample - loss: 0.6842 - accuracy: 0.7084 - precision_6: 0.7283 - recall_6: 0.6620 - val_loss: 0.6394 - val_accuracy: 0.7461 - val_precision_6: 0.7517 - val_recall_6: 0.7351\nEpoch 5/30\n4032/4074 [============================>.] - ETA: 0s - loss: 0.6386 - accuracy: 0.7202 - precision_6: 0.7372 - recall_6: 0.6845\nEpoch 00005: val_accuracy improved from 0.74614 to 0.77042, saving model to LSTMV3.h5\n4074/4074 [==============================] - 15s 4ms/sample - loss: 0.6376 - accuracy: 0.7207 - precision_6: 0.7378 - recall_6: 0.6853 - val_loss: 0.5810 - val_accuracy: 0.7704 - val_precision_6: 0.7855 - val_recall_6: 0.7439\nEpoch 6/30\n4032/4074 [============================>.] - ETA: 0s - loss: 0.6272 - accuracy: 0.7307 - precision_6: 0.7509 - recall_6: 0.6959\nEpoch 00006: val_accuracy did not improve from 0.77042\n4074/4074 [==============================] - 15s 4ms/sample - loss: 0.6272 - accuracy: 0.7310 - precision_6: 0.7514 - recall_6: 0.6959 - val_loss: 0.5741 - val_accuracy: 0.7616 - val_precision_6: 0.7788 - val_recall_6: 0.7461\nEpoch 7/30\n4032/4074 [============================>.] - ETA: 0s - loss: 0.5786 - accuracy: 0.7555 - precision_6: 0.7720 - recall_6: 0.7230\nEpoch 00007: val_accuracy did not improve from 0.77042\n4074/4074 [==============================] - 15s 4ms/sample - loss: 0.5791 - accuracy: 0.7553 - precision_6: 0.7717 - recall_6: 0.7226 - val_loss: 0.5690 - val_accuracy: 0.7550 - val_precision_6: 0.7674 - val_recall_6: 0.7285\nEpoch 8/30\n4032/4074 [============================>.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7614 - precision_6: 0.7799 - recall_6: 0.7366\nEpoch 00008: val_accuracy did not improve from 0.77042\n4074/4074 [==============================] - 15s 4ms/sample - loss: 0.5633 - accuracy: 0.7612 - precision_6: 0.7796 - recall_6: 0.7364 - val_loss: 0.5617 - val_accuracy: 0.7506 - val_precision_6: 0.7628 - val_recall_6: 0.7241\nEpoch 9/30\n4032/4074 [============================>.] - ETA: 0s - loss: 0.5223 - accuracy: 0.7803 - precision_6: 0.7956 - recall_6: 0.7547\nEpoch 00009: val_accuracy did not improve from 0.77042\n4074/4074 [==============================] - 15s 4ms/sample - loss: 0.5231 - accuracy: 0.7791 - precision_6: 0.7943 - recall_6: 0.7533 - val_loss: 0.5605 - val_accuracy: 0.7550 - val_precision_6: 0.7705 - val_recall_6: 0.7263\nEpoch 10/30\n4032/4074 [============================>.] - ETA: 0s - loss: 0.5111 - accuracy: 0.7825 - precision_6: 0.8025 - recall_6: 0.7639\nEpoch 00010: val_accuracy did not improve from 0.77042\n4074/4074 [==============================] - 15s 4ms/sample - loss: 0.5124 - accuracy: 0.7820 - precision_6: 0.8018 - recall_6: 0.7636 - val_loss: 0.5495 - val_accuracy: 0.7572 - val_precision_6: 0.7638 - val_recall_6: 0.7351\nEpoch 11/30\n4032/4074 [============================>.] - ETA: 0s - loss: 0.4927 - accuracy: 0.7959 - precision_6: 0.8115 - recall_6: 0.7728\nEpoch 00011: val_accuracy did not improve from 0.77042\n4074/4074 [==============================] - 14s 4ms/sample - loss: 0.4923 - accuracy: 0.7963 - precision_6: 0.8119 - recall_6: 0.7734 - val_loss: 0.5725 - val_accuracy: 0.7616 - val_precision_6: 0.7755 - val_recall_6: 0.7550\nEpoch 12/30\n4032/4074 [============================>.] - ETA: 0s - loss: 0.4677 - accuracy: 0.8016 - precision_6: 0.8158 - recall_6: 0.7852\nEpoch 00012: val_accuracy did not improve from 0.77042\n4074/4074 [==============================] - 15s 4ms/sample - loss: 0.4682 - accuracy: 0.8014 - precision_6: 0.8154 - recall_6: 0.7852 - val_loss: 0.5564 - val_accuracy: 0.7638 - val_precision_6: 0.7679 - val_recall_6: 0.7594\nEpoch 13/30\n4032/4074 [============================>.] - ETA: 0s - loss: 0.4470 - accuracy: 0.8105 - precision_6: 0.8248 - recall_6: 0.7964\nEpoch 00013: val_accuracy did not improve from 0.77042\n4074/4074 [==============================] - 15s 4ms/sample - loss: 0.4467 - accuracy: 0.8110 - precision_6: 0.8253 - recall_6: 0.7968 - val_loss: 0.5722 - val_accuracy: 0.7638 - val_precision_6: 0.7780 - val_recall_6: 0.7506\nEpoch 00013: early stopping\n"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]])\n",
    "\n",
    "initial_epochs = 30\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)\n",
    "mc=ModelCheckpoint('LSTMV3.h5', monitor='val_accuracy', mode='max', save_best_only=True,verbose=1) \n",
    "history = model.fit(train_sentence, train_emotion,batch_size=64, epochs=initial_epochs, validation_split=.1, verbose=1, callbacks=[es,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "564/564 [==============================] - 1s 1ms/sample - loss: 0.6753 - accuracy: 0.7181 - precision_6: 0.7320 - recall_6: 0.7021\n[0.675294392497827, 0.7180851, 0.7319778, 0.70212764]\n"
    }
   ],
   "source": [
    "model.load_weights('LSTMV3.h5')\n",
    "result = model.evaluate(test_sentence, test_emotion)\n",
    "yhat_class = model.predict_classes(test_sentence,verbose=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence = 'con đĩ mẹ mày'\n",
    "# labels = ['Anger', 'Disgust', 'Enjoyment', 'Fear', 'Other', 'Sadness', 'Surprise']\n",
    "# labels = ['Anger', 'Disgust', 'Enjoyment', 'Fear', 'Sadness', 'Surprise']\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "sentence = sentence_tokenizer.texts_to_sequences([sentence])\n",
    "sentence = sequence.pad_sequences(sentence, maxlen=MAX_LEN, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n",
    "pred = model.predict([sentence])\n",
    "print(pred) \n",
    "print(labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}